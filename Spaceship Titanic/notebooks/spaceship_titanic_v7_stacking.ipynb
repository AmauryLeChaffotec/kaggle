{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaceship Titanic - V7 (Stacking Ensemble)\n",
    "\n",
    "**Base:** V2 features (29 features, best LB 0.80710)  \n",
    "**Params:** Optuna-optimized from V6 (200 trials each)  \n",
    "**Ensemble:** Stacking with LogisticRegression meta-learner (replaces simple average)  \n",
    "\n",
    "**Stacking architecture:**\n",
    "1. Level 0: LightGBM, XGBoost, CatBoost → OOF predictions (10-fold)\n",
    "2. Level 1: LogisticRegression on OOF probas → final prediction (5-fold)\n",
    "3. Test predictions: average of level-1 test folds\n",
    "\n",
    "**Why stacking > simple average:**\n",
    "- Learns optimal blending weights per probability region\n",
    "- Can correct calibration differences between models\n",
    "- LogReg is linear = low overfit risk as meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings, os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 10\n",
    "TARGET = 'Transported'\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "print('V7 Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test[TARGET] = np.nan\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "print(f'Combined: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 Feature Engineering (identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === V2 FE ===\n",
    "df['_Group'] = df['PassengerId'].str.split('_').str[0].astype(int)\n",
    "df['GroupSize'] = df.groupby('_Group')['PassengerId'].transform('count')\n",
    "df['IsAlone'] = (df['GroupSize'] == 1).astype(int)\n",
    "\n",
    "df['Deck'] = df['Cabin'].str.split('/').str[0]\n",
    "df['CabinNum'] = df['Cabin'].str.split('/').str[1].astype(float)\n",
    "df['Side'] = df['Cabin'].str.split('/').str[2]\n",
    "df['CabinRegion'] = (df['CabinNum'] // 100).astype(float)\n",
    "\n",
    "df['Surname'] = df['Name'].str.split().str[-1]\n",
    "df['FamilySize'] = df.groupby('Surname')['PassengerId'].transform('count')\n",
    "df.loc[df['Surname'].isna(), 'FamilySize'] = 1\n",
    "\n",
    "df['CryoSleep'] = df['CryoSleep'].map({True: 1, False: 0, 'True': 1, 'False': 0})\n",
    "df['VIP'] = df['VIP'].map({True: 1, False: 0, 'True': 1, 'False': 0})\n",
    "\n",
    "# Imputation\n",
    "for col in spend_cols:\n",
    "    mask = (df['CryoSleep'] == 1) & (df[col].isna())\n",
    "    df.loc[mask, col] = 0\n",
    "mask = (df['CryoSleep'].isna()) & (df[spend_cols].sum(axis=1) == 0)\n",
    "df.loc[mask, 'CryoSleep'] = 1\n",
    "mask = (df['CryoSleep'].isna()) & (df[spend_cols].sum(axis=1) > 0)\n",
    "df.loc[mask, 'CryoSleep'] = 0\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Spending\n",
    "df['TotalSpend'] = df[spend_cols].sum(axis=1)\n",
    "df['TotalSpend_log'] = np.log1p(df['TotalSpend'])\n",
    "df['NoSpend'] = (df['TotalSpend'] == 0).astype(int)\n",
    "df['NumServicesUsed'] = (df[spend_cols] > 0).sum(axis=1)\n",
    "for col in spend_cols:\n",
    "    df[f'{col}_log'] = np.log1p(df[col])\n",
    "df['LuxurySpend'] = np.log1p(df['Spa'] + df['VRDeck'] + df['RoomService'])\n",
    "df['BasicSpend'] = np.log1p(df['FoodCourt'] + df['ShoppingMall'])\n",
    "\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=[0,5,12,18,30,50,80], labels=[0,1,2,3,4,5]).astype(float)\n",
    "df['IsChild'] = (df['Age'] < 18).astype(float)\n",
    "df['CryoSleep_NoSpend'] = ((df['CryoSleep'] == 1) & (df['TotalSpend'] == 0)).astype(int)\n",
    "\n",
    "df['GroupSpend_mean'] = df.groupby('_Group')['TotalSpend'].transform('mean')\n",
    "df['GroupSpend_mean_log'] = np.log1p(df['GroupSpend_mean'])\n",
    "\n",
    "for col in ['HomePlanet', 'Destination', 'Deck', 'Side']:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_le'] = le.fit_transform(df[col].astype(str))\n",
    "for col in ['HomePlanet', 'Destination', 'Deck', 'Side']:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    df[col + '_freq'] = df[col].map(freq)\n",
    "\n",
    "print('V2 FE done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'PassengerId', 'Name', 'Cabin', 'Surname', 'is_train', TARGET,\n",
    "    'HomePlanet', 'Destination', 'Deck', 'Side',\n",
    "    '_Group', 'CabinNum', 'TotalSpend', 'GroupSpend_mean',\n",
    "] + spend_cols\n",
    "\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "print(f'V2 features: {len(features)}')\n",
    "\n",
    "train_df = df[df['is_train'] == 1].copy()\n",
    "test_df = df[df['is_train'] == 0].copy()\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df[TARGET].astype(int).values\n",
    "X_test = test_df[features].values\n",
    "\n",
    "print(f'X: {X.shape}, y: {y.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 0: Base Models with Optuna Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna-optimized params from V6\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 44,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.030969372942932262,\n",
    "    'feature_fraction': 0.6576757841672172,\n",
    "    'bagging_fraction': 0.835862275982545,\n",
    "    'bagging_freq': 3,\n",
    "    'min_child_samples': 48,\n",
    "    'reg_alpha': 1.196564578792742,\n",
    "    'reg_lambda': 0.46422611416669396,\n",
    "    'min_split_gain': 0.1325484921651904,\n",
    "    'n_estimators': 5000,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03142745431117881,\n",
    "    'subsample': 0.7765649767697068,\n",
    "    'colsample_bytree': 0.6993827867384605,\n",
    "    'min_child_weight': 8,\n",
    "    'reg_alpha': 0.018176575713448703,\n",
    "    'reg_lambda': 0.0015265472174367683,\n",
    "    'gamma': 1.722059975175557,\n",
    "    'max_delta_step': 3,\n",
    "    'n_estimators': 5000,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': SEED,\n",
    "    'verbosity': 0,\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.03873109015756435,\n",
    "    'depth': 7,\n",
    "    'l2_leaf_reg': 0.21548007012285542,\n",
    "    'subsample': 0.9396092698948957,\n",
    "    'colsample_bylevel': 0.5062767287275375,\n",
    "    'min_data_in_leaf': 16,\n",
    "    'random_strength': 3.2715423389693354,\n",
    "    'bagging_temperature': 0.8108454420073308,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 500,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'task_type': 'CPU',\n",
    "}\n",
    "\n",
    "print('Optuna params loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Level 0: LightGBM ===\n",
    "oof_lgb = np.zeros(len(X))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "fi_lgb = np.zeros(len(features))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(500)]\n",
    "    )\n",
    "    \n",
    "    oof_lgb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_lgb += model.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    fi_lgb += model.feature_importances_ / N_FOLDS\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_lgb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS} - LGB Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "lgb_acc = accuracy_score(y, (oof_lgb > 0.5).astype(int))\n",
    "print(f'\\nLightGBM 10-fold CV: {lgb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Level 0: XGBoost ===\n",
    "oof_xgb = np.zeros(len(X))\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=500)\n",
    "    \n",
    "    oof_xgb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_xgb += model.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_xgb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS} - XGB Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "xgb_acc = accuracy_score(y, (oof_xgb > 0.5).astype(int))\n",
    "print(f'\\nXGBoost 10-fold CV: {xgb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Level 0: CatBoost ===\n",
    "oof_cb = np.zeros(len(X))\n",
    "test_cb = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**cb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "    \n",
    "    oof_cb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_cb += model.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_cb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS} - CB Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "cb_acc = accuracy_score(y, (oof_cb > 0.5).astype(int))\n",
    "print(f'\\nCatBoost 10-fold CV: {cb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Level 0 Results ===')\n",
    "print(f'LightGBM: {lgb_acc:.5f}')\n",
    "print(f'XGBoost:  {xgb_acc:.5f}')\n",
    "print(f'CatBoost: {cb_acc:.5f}')\n",
    "\n",
    "# Simple average baseline (for comparison)\n",
    "oof_avg = (oof_lgb + oof_xgb + oof_cb) / 3\n",
    "avg_acc = accuracy_score(y, (oof_avg > 0.5).astype(int))\n",
    "print(f'\\nSimple Average (baseline): {avg_acc:.5f}')\n",
    "\n",
    "# Check OOF correlation (diversity = better stacking)\n",
    "corr_lgb_xgb = np.corrcoef(oof_lgb, oof_xgb)[0, 1]\n",
    "corr_lgb_cb = np.corrcoef(oof_lgb, oof_cb)[0, 1]\n",
    "corr_xgb_cb = np.corrcoef(oof_xgb, oof_cb)[0, 1]\n",
    "print(f'\\nOOF Correlation (lower = more diverse = better stacking):')\n",
    "print(f'  LGB-XGB: {corr_lgb_xgb:.4f}')\n",
    "print(f'  LGB-CB:  {corr_lgb_cb:.4f}')\n",
    "print(f'  XGB-CB:  {corr_xgb_cb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: LogisticRegression Stacking\n",
    "\n",
    "The meta-learner sees the 3 OOF probability columns and learns optimal blending.  \n",
    "We use 5-fold CV on the meta-learner to avoid data leakage.  \n",
    "We test multiple C values (regularization) to find the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stacking matrices\n",
    "oof_stack = np.column_stack([oof_lgb, oof_xgb, oof_cb])\n",
    "test_stack = np.column_stack([test_lgb, test_xgb, test_cb])\n",
    "\n",
    "print(f'OOF stack shape: {oof_stack.shape}')\n",
    "print(f'Test stack shape: {test_stack.shape}')\n",
    "\n",
    "# Try multiple C values\n",
    "print('\\n=== LogReg Meta-Learner: C search ===')\n",
    "best_c = None\n",
    "best_stack_acc = 0\n",
    "\n",
    "for C in [0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]:\n",
    "    kf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    meta_oof = np.zeros(len(y))\n",
    "    \n",
    "    for tr_idx, val_idx in kf_meta.split(oof_stack, y):\n",
    "        meta = LogisticRegression(C=C, random_state=SEED, max_iter=1000)\n",
    "        meta.fit(oof_stack[tr_idx], y[tr_idx])\n",
    "        meta_oof[val_idx] = meta.predict_proba(oof_stack[val_idx])[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y, (meta_oof > 0.5).astype(int))\n",
    "    print(f'  C={C:6.2f} -> Stacking CV: {acc:.5f}')\n",
    "    \n",
    "    if acc > best_stack_acc:\n",
    "        best_stack_acc = acc\n",
    "        best_c = C\n",
    "\n",
    "print(f'\\nBest C: {best_c}, Best Stacking CV: {best_stack_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final stacking with best C ===\n",
    "kf_meta = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "meta_oof_final = np.zeros(len(y))\n",
    "meta_test_final = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf_meta.split(oof_stack, y)):\n",
    "    meta = LogisticRegression(C=best_c, random_state=SEED, max_iter=1000)\n",
    "    meta.fit(oof_stack[tr_idx], y[tr_idx])\n",
    "    \n",
    "    meta_oof_final[val_idx] = meta.predict_proba(oof_stack[val_idx])[:, 1]\n",
    "    meta_test_final += meta.predict_proba(test_stack)[:, 1] / 5\n",
    "    \n",
    "    fold_acc = accuracy_score(y[val_idx], (meta_oof_final[val_idx] > 0.5).astype(int))\n",
    "    print(f'Meta Fold {fold+1}/5 - Accuracy: {fold_acc:.5f}')\n",
    "    \n",
    "    # Show weights (coefficients)\n",
    "    print(f'  Weights: LGB={meta.coef_[0][0]:.3f}, XGB={meta.coef_[0][1]:.3f}, CB={meta.coef_[0][2]:.3f}')\n",
    "\n",
    "stack_acc = accuracy_score(y, (meta_oof_final > 0.5).astype(int))\n",
    "print(f'\\nStacking CV Accuracy: {stack_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also compute majority voting for reference\n",
    "votes = ((oof_lgb > 0.5).astype(int) + (oof_xgb > 0.5).astype(int) + (oof_cb > 0.5).astype(int))\n",
    "vote_acc = accuracy_score(y, (votes >= 2).astype(int))\n",
    "\n",
    "print('='*60)\n",
    "print('ENSEMBLE COMPARISON (V7)')\n",
    "print('='*60)\n",
    "print(f'\\nLevel 0 Individual Models:')\n",
    "print(f'  LightGBM (Optuna):  {lgb_acc:.5f}')\n",
    "print(f'  XGBoost (Optuna):   {xgb_acc:.5f}')\n",
    "print(f'  CatBoost (Optuna):  {cb_acc:.5f}')\n",
    "print(f'\\nEnsemble Methods:')\n",
    "print(f'  Simple Average:     {avg_acc:.5f}')\n",
    "print(f'  Majority Voting:    {vote_acc:.5f}')\n",
    "print(f'  Stacking (LogReg):  {stack_acc:.5f}  <-- C={best_c}')\n",
    "print(f'\\nStacking vs Average:  {stack_acc - avg_acc:+.5f}')\n",
    "\n",
    "print(f'\\n=== VERSION COMPARISON ===')\n",
    "print(f'V1:  CV 0.82653 | LB 0.80196 | Gap 0.0246 | simple avg')\n",
    "print(f'V2:  CV 0.81410 | LB 0.80710 | Gap 0.0070 | simple avg')\n",
    "print(f'V3:  CV 0.81836 | LB 0.80406 | Gap 0.0143 | simple avg')\n",
    "print(f'V5:  CV 0.81767 | LB TBD     |            | simple avg + TE')\n",
    "print(f'V6:  CV 0.81652 | LB TBD     |            | simple avg + Optuna')\n",
    "print(f'V7:  CV {stack_acc:.5f} | LB TBD     |            | STACKING + Optuna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple submissions for comparison\n",
    "\n",
    "# Submission 1: Stacking\n",
    "final_preds_stack = (meta_test_final > 0.5)\n",
    "sub_stack = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'].values,\n",
    "    'Transported': final_preds_stack\n",
    "})\n",
    "sub_stack['Transported'] = sub_stack['Transported'].astype(bool)\n",
    "sub_stack.to_csv('../submissions/submission_v7_stacking.csv', index=False)\n",
    "\n",
    "# Submission 2: Simple average (same models, for A/B comparison)\n",
    "final_preds_avg = ((test_lgb + test_xgb + test_cb) / 3 > 0.5)\n",
    "sub_avg = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'].values,\n",
    "    'Transported': final_preds_avg\n",
    "})\n",
    "sub_avg['Transported'] = sub_avg['Transported'].astype(bool)\n",
    "sub_avg.to_csv('../submissions/submission_v7_average.csv', index=False)\n",
    "\n",
    "# Validate\n",
    "for name, sub in [('stacking', sub_stack), ('average', sub_avg)]:\n",
    "    assert sub.shape[0] == sample_sub.shape[0]\n",
    "    assert list(sub.columns) == list(sample_sub.columns)\n",
    "    assert sub['Transported'].dtype == bool\n",
    "\n",
    "# Compare predictions\n",
    "diff = (final_preds_stack != final_preds_avg).sum()\n",
    "print(f'Stacking vs Average: {diff} different predictions ({diff/len(final_preds_stack)*100:.1f}%)')\n",
    "print(f'\\nStacking: {final_preds_stack.sum()} True ({final_preds_stack.mean():.4f})')\n",
    "print(f'Average:  {final_preds_avg.sum()} True ({final_preds_avg.mean():.4f})')\n",
    "print(f'\\nSaved: submission_v7_stacking.csv, submission_v7_average.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
