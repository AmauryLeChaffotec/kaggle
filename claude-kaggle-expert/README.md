# Claude Code - Expert Kaggle Gold Medal

34 skills + 4 agents pour transformer Claude Code en expert Kaggle. Couvre tout le workflow : de l'exploration des donnÃ©es Ã  la soumission finale.

Tu n'as pas besoin d'Ãªtre un expert en data science. Suis ce guide Ã©tape par Ã©tape, lance les commandes, et Claude fait le travail.

---

## Vocabulaire (pour les dÃ©butants)

Avant de commencer, voici les termes que tu vas croiser :

| Terme | Signification |
|-------|--------------|
| **Skill** (`/kaggle-*`) | Une commande que tu tapes dans Claude Code. Elle fait une tÃ¢che prÃ©cise (nettoyer les donnÃ©es, entraÃ®ner un modÃ¨le, etc.) |
| **Agent** | Un assistant autonome qui fait une mission longue (analyser une compÃ©tition, optimiser un modÃ¨le). Plus puissant qu'un skill. |
| **CV (Cross-Validation)** | Le score de ton modÃ¨le mesurÃ© en local, sur tes propres donnÃ©es. C'est ta boussole. |
| **LB (Leaderboard)** | Le score affichÃ© par Kaggle quand tu soumets. C'est le vrai score. |
| **Gap CV-LB** | La diffÃ©rence entre ton score local (CV) et le score Kaggle (LB). C'est **le** indicateur le plus important : un gap < 3% = ton CV est fiable et tu peux l'utiliser pour prendre des dÃ©cisions. Un gap > 5% = danger, ton CV te ment. |
| **OOF (Out-Of-Fold)** | Les prÃ©dictions de ton modÃ¨le sur les donnÃ©es d'entraÃ®nement, faites fold par fold (le modÃ¨le ne voit jamais les donnÃ©es qu'il prÃ©dit). Sert Ã  calculer le CV et Ã  construire les ensembles. |
| **Feature** | Une colonne dans tes donnÃ©es. "Feature engineering" = crÃ©er de nouvelles colonnes utiles. |
| **Baseline** | Un premier modÃ¨le trÃ¨s simple, sans optimisation, juste pour avoir un score de dÃ©part. |
| **Ensemble** | Combiner les prÃ©dictions de plusieurs modÃ¨les pour avoir un meilleur score. |
| **Overfitting** | Le modÃ¨le a "mÃ©morisÃ©" les donnÃ©es d'entraÃ®nement au lieu d'apprendre des patterns. Score de train excellent mais score de validation mauvais. |
| **Leakage** | Le modÃ¨le a accÃ¨s Ã  de l'information qu'il ne devrait pas avoir (ex: la rÃ©ponse est cachÃ©e dans une feature). Le score semble trÃ¨s bon mais c'est faux. |
| **SHAP** | Une technique pour comprendre quelles features sont importantes pour le modÃ¨le et pourquoi. |
| **GBDT** | Gradient Boosted Decision Trees : LightGBM, XGBoost, CatBoost. Les modÃ¨les qui gagnent la majoritÃ© des compÃ©titions tabulaires. |

---

## Installation

```bash
# Installation globale (disponible dans tous tes projets)
cp -r skills/* ~/.claude/skills/
cp -r agents/* ~/.claude/agents/
```

Puis **redÃ©marre Claude Code** pour activer les skills et agents.

> **Perdu Ã  n'importe quel moment ?** Tape `/kaggle-guide`. Il analyse ton projet et te dit exactement quoi faire ensuite, avec la commande Ã  copier-coller.

---

## Le Workflow complet â€” De zÃ©ro Ã  la mÃ©daille d'or

La clÃ© pour gagner sur Kaggle, c'est l'**itÃ©ration**. Tu ne fais pas tout une seule fois dans l'ordre. Tu fais des boucles : tu crÃ©es des features, tu entraÃ®nes, tu Ã©values, tu ajustes, tu recommences. Chaque boucle amÃ©liore ton score un petit peu.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                   â”‚
â”‚  Ã‰TAPE 1 â€” Comprendre la compÃ©tition            â± 1-2 heures    â”‚
â”‚  Ã‰TAPE 2 â€” PremiÃ¨re soumission (baseline)        â± 1-3 heures    â”‚
â”‚                                                                   â”‚
â”‚         â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                 â”‚
â”‚         â•‘        BOUCLE D'ITÃ‰RATION             â•‘                 â”‚
â”‚         â•‘   (tu passes 80% de ton temps ici)    â•‘                 â”‚
â”‚         â•‘                                        â•‘                 â”‚
â”‚         â•‘  Ã‰TAPE 3 â€” AmÃ©liorer les features      â•‘  â± 30 min     â”‚
â”‚         â•‘  Ã‰TAPE 4 â€” EntraÃ®ner le modÃ¨le         â•‘  â± 15-45 min  â”‚
â”‚         â•‘  Ã‰TAPE 5 â€” Ã‰valuer et comparer         â•‘  â± 10 min     â”‚
â”‚         â•‘              â†“                         â•‘                 â”‚
â”‚         â•‘  Le score monte ? â†’ Continuer.         â•‘                 â”‚
â”‚         â•‘  Le score stagne ? â†’ Changer.          â•‘                 â”‚
â”‚         â•‘              â†“                         â•‘                 â”‚
â”‚         â•‘      Retour Ã  l'Ã©tape 3                â•‘                 â”‚
â”‚         â•‘                                        â•‘                 â”‚
â”‚         â•‘  ğŸ’¡ Chaque boucle = ~1 heure           â•‘                 â”‚
â”‚         â•‘     PrÃ©voir 10-30 boucles par compet.  â•‘                 â”‚
â”‚         â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                 â”‚
â”‚                                                                   â”‚
â”‚  Ã‰TAPE 6 â€” Combiner les modÃ¨les (ensemble)       â± 1-2 heures    â”‚
â”‚  Ã‰TAPE 7 â€” VÃ©rifier et soumettre                 â± 1-2 heures    â”‚
â”‚                                                                   â”‚
â”‚  TOTAL pour une compÃ©tition : 2-4 semaines Ã  raison de            â”‚
â”‚  quelques heures par jour (plus si compÃ©tition complexe)          â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ã‰TAPE 1 â€” Comprendre la compÃ©tition (â± 1-2 heures)

**Objectif :** Savoir Ã  quoi tu as affaire avant d'Ã©crire du code.

### Option A : Demander un plan stratÃ©gique (recommandÃ© pour les dÃ©butants)

```
â†’ Agent kaggle-strategist
  "Analyse la compÃ©tition Spaceship Titanic et crÃ©e un plan d'attaque"
```

**Ce que Ã§a fait :** L'agent va :
- Lire les donnÃ©es de la compÃ©tition
- Rechercher sur le web les solutions gagnantes de compÃ©titions similaires
- Identifier le type de problÃ¨me, la mÃ©trique, les piÃ¨ges
- Produire un plan multi-phases avec des scores attendus

**Ce que tu obtiens :** Un rapport complet dans `reports/strategy/` avec un plan Ã  suivre.

### Option B : Explorer les donnÃ©es directement

```
â†’ /kaggle-eda data/train.csv
```

**Ce que Ã§a fait :** Analyse exploratoire complÃ¨te en 10 Ã©tapes :
- Quels types de colonnes (nombres, texte, dates) ?
- Combien de valeurs manquantes ?
- Comment est distribuÃ© le target (ce qu'on veut prÃ©dire) ?
- Quelles colonnes sont corrÃ©lÃ©es entre elles ?
- Y a-t-il des outliers (valeurs anormales) ?
- Les donnÃ©es de train et test sont-elles similaires ?

**Ce que tu obtiens :** Un rapport dans `reports/eda/` qui te dit tout ce qu'il faut savoir sur tes donnÃ©es.

### Si tu as besoin de recherches

```
â†’ Agent kaggle-researcher
  "Cherche les meilleures techniques pour la classification tabulaire binaire"
```

**Ce que Ã§a fait :** Recherche sur le web les techniques, notebooks populaires, et solutions gagnantes. Rapport dans `reports/research/`.

---

## Ã‰TAPE 2 â€” PremiÃ¨re soumission (le baseline) (â± 1-3 heures)

**Objectif :** Avoir un premier score sur le Leaderboard le plus vite possible. Ce score sert de rÃ©fÃ©rence : tout ce que tu fais ensuite doit le battre.

### Option A : Pipeline complet d'un coup (le plus rapide)

```
â†’ /kaggle-pipeline Spaceship Titanic
```

**Ce que Ã§a fait :** Tout d'un coup :
- CrÃ©e la structure du projet (dossiers data/, models/, submissions/...)
- Fait une EDA rapide
- Nettoie les donnÃ©es
- CrÃ©e quelques features de base
- EntraÃ®ne un modÃ¨le LightGBM
- PrÃ©pare une premiÃ¨re soumission

**Ce que tu obtiens :** Un projet complet prÃªt Ã  itÃ©rer + une soumission Ã  envoyer.

### Option B : Baseline minimaliste

```
â†’ /kaggle-baseline
```

**Ce que Ã§a fait :** CrÃ©e un modÃ¨le ultra simple en moins de 30 minutes. Pas de feature engineering, paramÃ¨tres par dÃ©faut. Juste le strict minimum pour avoir un score.

### Soumettre ton baseline

```
â†’ /kaggle-submit
```

**Ce que Ã§a fait :** VÃ©rifie que ton fichier de soumission est correct (bonnes colonnes, bon nombre de lignes, pas de NaN) et le prÃ©pare.

### Comparer CV et LB (trÃ¨s important !)

AprÃ¨s ta premiÃ¨re soumission, note les deux scores :
- **CV** : le score calculÃ© en local (affichÃ© pendant l'entraÃ®nement)
- **LB** : le score affichÃ© par Kaggle aprÃ¨s soumission

| Gap CV-LB | Ce que Ã§a veut dire | Quoi faire |
|-----------|--------------------|-----------|
| < 3% | Ton CV est fiable. Tu peux l'utiliser comme boussole. | Continue normalement |
| 3-5% | Prudence. Le CV est approximatif. | Lance `/kaggle-validation` pour amÃ©liorer ta stratÃ©gie de CV |
| > 5% | ProblÃ¨me. Ton CV n'est pas fiable du tout. | Lance `/kaggle-leakage` (tu as peut-Ãªtre du leakage) puis `/kaggle-validation` |

---

## Ã‰TAPE 3 â€” AmÃ©liorer les features (â± 30 min par itÃ©ration)

**Objectif :** CrÃ©er de nouvelles colonnes dans tes donnÃ©es qui aident le modÃ¨le Ã  mieux prÃ©dire. C'est l'Ã©tape qui a le **plus d'impact** sur ton score.

### Nettoyer les donnÃ©es (si pas encore fait)

```
â†’ /kaggle-cleaning
```

**Ce que Ã§a fait :**
- Corrige les types de donnÃ©es (un nombre stockÃ© comme texte, etc.)
- Traite les valeurs manquantes (les remplace intelligemment)
- DÃ©tecte les NaN dÃ©guisÃ©s (des "N/A", "?", "-", -999 qui cachent des valeurs manquantes)
- Supprime les doublons
- GÃ¨re les outliers (valeurs extrÃªmes)
- Regroupe les catÃ©gories rares
- Supprime les colonnes constantes (qui ne servent Ã  rien)

**Ce que tu obtiens :** Des donnÃ©es propres + un rapport dans `reports/cleaning/`.

### CrÃ©er des features

```
â†’ /kaggle-feature
```

**Ce que Ã§a fait :**
- CrÃ©e des interactions entre colonnes (A Ã— B, A / B, A - B)
- CrÃ©e des agrÃ©gations (moyenne par groupe, count, etc.)
- Encode les catÃ©gories (frequency encoding, target encoding)
- CrÃ©e des features temporelles si applicable (jour, mois, heure)
- Mesure l'impact de chaque feature sur le CV

**Ce que tu obtiens :** De nouvelles features + un rapport avec l'impact de chacune.

### Si tu veux visualiser

```
â†’ /kaggle-viz
```

**Ce que Ã§a fait :** CrÃ©e des graphiques avancÃ©s (distributions, corrÃ©lations, heatmaps) pour mieux comprendre les donnÃ©es.

---

## Ã‰TAPE 4 â€” EntraÃ®ner le modÃ¨le (â± 15-45 min par itÃ©ration)

**Objectif :** EntraÃ®ner un modÃ¨le avec tes nouvelles features et mesurer si le score s'est amÃ©liorÃ©.

```
â†’ /kaggle-model
```

**Ce que Ã§a fait :**
- EntraÃ®ne un ou plusieurs modÃ¨les (LightGBM, XGBoost, CatBoost)
- Utilise la cross-validation (5 folds par dÃ©faut) pour un score fiable
- Sauvegarde les prÃ©dictions OOF et test dans `artifacts/`
- Affiche l'importance des features

**Pour des donnÃ©es spÃ©cifiques :**

| Tes donnÃ©es sont... | Lance... |
|---------------------|----------|
| Un tableau CSV classique | `/kaggle-model` ou `/kaggle-tabular` |
| Du texte (reviews, articles, tweets) | `/kaggle-nlp` |
| Des images (photos, scans, satellite) | `/kaggle-cv` |
| Des sÃ©ries temporelles (ventes, mÃ©tÃ©o, finance) | `/kaggle-timeseries` |
| Un tableau mais tu veux du deep learning | `/kaggle-deeplearning` |
| Un jeu ou une simulation | `/kaggle-rl` |

---

## Ã‰TAPE 5 â€” Ã‰valuer et comparer (â± 10 min par itÃ©ration)

**Objectif :** Savoir si ton changement a amÃ©liorÃ© le score ou non. Ne garder que ce qui marche.

```
â†’ /kaggle-experiments
```

**Ce que Ã§a fait :**
- Log ton run dans `runs.csv` (CV score, features utilisÃ©es, paramÃ¨tres)
- Compare avec les runs prÃ©cÃ©dents
- Identifie quel changement a eu le plus d'impact

### DÃ©cider quoi faire ensuite

| RÃ©sultat | Ce que Ã§a veut dire | Quoi faire |
|----------|--------------------|-----------|
| CV monte de +0.005 ou plus | Ta feature ou ton changement marche | Garde-le ! Retour Ã  l'Ã©tape 3 pour ajouter d'autres features |
| CV ne bouge pas | Le changement n'apporte rien | Supprime-le. Essaie une autre approche |
| CV baisse | Bug ou bruit | Lance `/kaggle-debug` pour comprendre pourquoi |
| CV monte mais LB baisse | Overfitting probable | Lance `/kaggle-validation` pour revoir le CV |
| Tu ne sais pas quelles features sont utiles | Besoin d'analyse | Lance `/kaggle-explain` (SHAP values) |

### Comprendre ton modÃ¨le

```
â†’ /kaggle-explain
```

**Ce que Ã§a fait :** Montre quelles features sont importantes, lesquelles le modÃ¨le utilise vraiment, et lesquelles sont du bruit. Te montre aussi les Partial Dependence Plots (comment chaque feature influence la prÃ©diction).

### Quand itÃ©rer ? Quand passer Ã  l'ensemble ?

**Continue d'itÃ©rer (Ã©tapes 3-4-5) tant que :**
- Tu trouves des features qui amÃ©liorent le score
- Tu n'as pas encore testÃ© les approches Ã©videntes
- Tu as des idÃ©es Ã  explorer

**Passe Ã  l'Ã©tape 6 quand :**
- Le score ne monte plus malgrÃ© tes efforts
- Tu as au moins 2-3 modÃ¨les diffÃ©rents (LightGBM, XGBoost, CatBoost)
- Tu as essayÃ© les principales features

---

## Quand Ã§a va mal (Ã  utiliser pendant les itÃ©rations)

### Le score a baissÃ©

```
â†’ Agent kaggle-debugger
  "Mon score est passÃ© de 0.81 Ã  0.79 aprÃ¨s avoir ajoutÃ© des features d'interaction"
```

**Ce que Ã§a fait :** Le debugger analyse ton code, tes donnÃ©es, et tes rÃ©sultats. Il produit un **diagnostic** (la cause du problÃ¨me) et un **patch plan** (les fichiers Ã  modifier avec le diff exact ligne par ligne). Rapport dans `reports/debug/`.

### Le score stagne

```
â†’ /kaggle-debug
  "Le score stagne Ã  0.81 depuis 5 itÃ©rations"
```

**Ce que Ã§a fait :** Analyse les prÃ©dictions du modÃ¨le, identifie les observations les plus mal prÃ©dites, et propose des pistes d'amÃ©lioration.

Ensuite :
```
â†’ Agent kaggle-researcher
  "Cherche des techniques avancÃ©es pour amÃ©liorer une classification binaire tabulaire"
```

**Ce que Ã§a fait :** Recherche sur le web des nouvelles idÃ©es et techniques.

### Le CV et le LB ne matchent pas

```
â†’ /kaggle-validation
```

**Ce que Ã§a fait :** Diagnostique ta stratÃ©gie de CV. Peut-Ãªtre que tu as besoin de GroupKFold (donnÃ©es groupÃ©es), TimeSeriesCV (donnÃ©es temporelles), ou adversarial validation.

### Score suspicieusement bon

```
â†’ /kaggle-leakage
```

**Ce que Ã§a fait :** Audit complet de data leakage en 7 points. Si une feature a un score trop bon, c'est peut-Ãªtre parce qu'elle contient la rÃ©ponse.

### C'est trop lent ou RAM saturÃ©e

```
â†’ /kaggle-efficiency
```

**Ce que Ã§a fait :** RÃ©duit l'usage mÃ©moire, accÃ©lÃ¨re le training, propose des alternatives (Polars au lieu de Pandas, GPU, etc.).

### Pas assez de donnÃ©es ou classes dÃ©sÃ©quilibrÃ©es

```
â†’ /kaggle-augmentation
```

**Ce que Ã§a fait :** Augmente tes donnÃ©es avec SMOTE (pour le dÃ©sÃ©quilibre), Mixup, pseudo-labeling, etc.

### Optimiser les hyperparamÃ¨tres

```
â†’ Agent kaggle-optimizer
  "Optimise les hyperparamÃ¨tres de mon LightGBM"
```

**Ce que Ã§a fait :** CrÃ©e un script Optuna, lance une recherche automatique des meilleurs paramÃ¨tres, et sauvegarde les rÃ©sultats dans `configs/` et `reports/optimizer/`.

> **Important :** Ne fais le tuning que quand tes features sont stables. Les features ont plus d'impact que les hyperparamÃ¨tres.

---

## Ã‰TAPE 6 â€” Combiner les modÃ¨les (ensemble) (â± 1-2 heures)

**Objectif :** Combiner les prÃ©dictions de plusieurs modÃ¨les pour obtenir un meilleur score que chaque modÃ¨le individuellement.

**PrÃ©requis :** Tu dois avoir au moins 2-3 modÃ¨les diffÃ©rents (ex: LightGBM + XGBoost + CatBoost).

```
â†’ /kaggle-ensemble
```

**Ce que Ã§a fait :**
- Calcule la corrÃ©lation entre tes modÃ¨les (s'ils prÃ©disent tous la mÃªme chose, l'ensemble n'aide pas)
- Teste diffÃ©rentes mÃ©thodes : moyenne simple, moyenne pondÃ©rÃ©e, rank average, stacking
- Trouve les poids optimaux
- Sauvegarde les prÃ©dictions de l'ensemble dans `artifacts/`

| CorrÃ©lation entre modÃ¨les | Ce que Ã§a veut dire | Quoi faire |
|--------------------------|--------------------|-----------|
| > 0.98 | Les modÃ¨les sont trop similaires | Diversifie : `/kaggle-deeplearning`, features diffÃ©rentes, seeds diffÃ©rents |
| 0.93 - 0.97 | Bonne diversitÃ© | L'ensemble va bien marcher |
| < 0.93 | TrÃ¨s diffÃ©rents (attention) | VÃ©rifie que chaque modÃ¨le est bon individuellement |

---

## Ã‰TAPE 7 â€” VÃ©rifier et soumettre (â± 1-2 heures)

### 7a. Polish (pour gratter les derniers points)

```
â†’ /kaggle-metrics
```
VÃ©rifie que ta mÃ©trique locale correspond exactement Ã  celle de Kaggle.

```
â†’ /kaggle-calibration
```
Calibre les probabilitÃ©s. Utile si la mÃ©trique est Log Loss ou Brier Score.

```
â†’ /kaggle-postprocess
```
Optimise les seuils de dÃ©cision (pour F1, Accuracy), le clipping, l'arrondi.

```
â†’ /kaggle-inference
```
Pipeline d'infÃ©rence optimisÃ©. Utile pour les "code competitions" avec contrainte de temps.

### 7b. VÃ©rification finale

```
â†’ /kaggle-sanity
```

**Ce que Ã§a fait :** Suite complÃ¨te de tests de sanitÃ© :
- Le fichier de soumission a le bon format ?
- Pas de NaN dans les prÃ©dictions ?
- Les features sont-elles vraiment utiles (vs features alÃ©atoires) ?
- Le preprocessing est-il bien dans le fold (pas de leakage) ?

**Lance toujours `/kaggle-sanity` avant une soumission importante.**

### 7c. Choisir les 2 soumissions finales

```
â†’ /kaggle-leaderboard
```

**Ce que Ã§a fait :** Analyse le risque de "shake-up" (quand le classement change entre le public LB et le private LB) et t'aide Ã  choisir tes 2 soumissions finales.

**RÃ¨gle simple pour choisir :**
| Soumission 1 (conservative) | Soumission 2 (aggressive) |
|-----------------------------|--------------------------|
| Le modÃ¨le avec le meilleur CV et le plus stable | Le meilleur ensemble ou la meilleure expÃ©rimentation |

### 7d. Soumettre

```
â†’ /kaggle-submit
```

**Ce que Ã§a fait :** Valide le format et prÃ©pare le fichier `submission.csv`.

---

## RÃ©capitulatif â€” Toutes les commandes

### Les 4 Agents (missions longues et autonomes)

| Agent | Ce qu'il fait | Quand l'utiliser |
|-------|---------------|-----------------|
| `kaggle-strategist` | Analyse la compÃ©tition, recherche les solutions gagnantes similaires sur le web, produit un plan multi-phases | Tu commences une compÃ©tition |
| `kaggle-researcher` | Recherche des techniques, notebooks populaires, et solutions gagnantes sur le web | Tu cherches de nouvelles idÃ©es |
| `kaggle-optimizer` | CrÃ©e des scripts Optuna et optimise les hyperparamÃ¨tres automatiquement | Tes features sont stables, tu veux tuner |
| `kaggle-debugger` | Diagnostique un problÃ¨me et produit un patch plan (fichiers + lignes + diff) | Le score a baissÃ© ou quelque chose ne va pas |

### Les 34 Skills (commandes rapides)

#### Navigation
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-guide` | **T'aide quand tu es perdu.** Analyse ton projet et te dit quoi faire avec la commande exacte. |
| `/kaggle-knowledge` | Base de connaissances chargÃ©e automatiquement. Tu n'as rien Ã  faire. |

#### DÃ©marrage
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-pipeline` | CrÃ©e un projet complet de A Ã  Z : structure + EDA + cleaning + features + modÃ¨le + soumission. |
| `/kaggle-baseline` | CrÃ©e un modÃ¨le ultra simple en <30 min. Juste pour avoir un premier score. |

#### DonnÃ©es
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-eda` | Analyse exploratoire complÃ¨te des donnÃ©es (types, distributions, corrÃ©lations, missing values). |
| `/kaggle-cleaning` | Nettoie les donnÃ©es : types, missing values, outliers, doublons, NaN dÃ©guisÃ©s, catÃ©gories rares. |
| `/kaggle-feature` | CrÃ©e des features : interactions, agrÃ©gations, encodages, features temporelles. |
| `/kaggle-viz` | CrÃ©e des graphiques avancÃ©s avec Seaborn, Matplotlib ou Plotly. |
| `/kaggle-leakage` | DÃ©tecte le data leakage en 7 points. Lance Ã§a si ton score semble trop beau. |
| `/kaggle-augmentation` | Augmente les donnÃ©es : SMOTE, Mixup, pseudo-labeling. Pour les petits datasets ou classes dÃ©sÃ©quilibrÃ©es. |

#### ModÃ©lisation
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-model` | EntraÃ®ne des modÃ¨les (LightGBM, XGBoost, CatBoost) avec cross-validation. |
| `/kaggle-validation` | Choisit la bonne stratÃ©gie de CV. Diagnostique les problÃ¨mes CV vs LB. |
| `/kaggle-ensemble` | Combine plusieurs modÃ¨les (moyenne, stacking, blending). Trouve les poids optimaux. |
| `/kaggle-experiments` | Tracker d'expÃ©riences : log chaque run, compare les scores, identifie ce qui marche. |

#### Diagnostic
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-debug` | Diagnostique les problÃ¨mes : overfitting, score qui baisse, erreurs de prÃ©diction. |
| `/kaggle-explain` | Explique le modÃ¨le : SHAP, permutation importance. Montre quelles features comptent. |
| `/kaggle-sanity` | Tests de sanitÃ© avant soumission : format, NaN, leakage rÃ©siduel, features utiles. |
| `/kaggle-metrics` | VÃ©rifie que ta mÃ©trique locale = mÃ©trique Kaggle. ImplÃ©mente des mÃ©triques custom. |

#### Finalisation
| Commande | Ce qu'elle fait |
|----------|----------------|
| `/kaggle-calibration` | Calibre les probabilitÃ©s (Platt, Isotonic). Pour Log Loss / Brier Score. |
| `/kaggle-postprocess` | Optimise les seuils, clipping, arrondi. Pour F1, Accuracy, QWK. |
| `/kaggle-inference` | Pipeline d'infÃ©rence optimisÃ© (TTA, batch, multi-model). Pour les code competitions. |
| `/kaggle-leaderboard` | Analyse le risque de shake-up. Aide Ã  choisir les 2 soumissions finales. |
| `/kaggle-submit` | Valide le format de soumission et prÃ©pare le fichier final. |

#### Domaines spÃ©cialisÃ©s
| Commande | SpÃ©cialitÃ© |
|----------|-----------|
| `/kaggle-tabular` | DonnÃ©es tabulaires (CSV classique) â€” pipeline complet optimisÃ© |
| `/kaggle-nlp` | Texte â€” fine-tuning BERT/DeBERTa, classification, NER |
| `/kaggle-cv` | Images â€” EfficientNet, ViT, augmentations, segmentation |
| `/kaggle-timeseries` | SÃ©ries temporelles â€” lag features, Prophet, validation temporelle |
| `/kaggle-deeplearning` | Deep learning sur tableaux â€” TabNet, FT-Transformer, SAINT |
| `/kaggle-sql` | SQL et BigQuery â€” requÃªtes, window functions, optimisation |
| `/kaggle-geospatial` | DonnÃ©es gÃ©ographiques â€” GeoPandas, Folium, distance features |
| `/kaggle-rl` | Jeux et simulation â€” minimax, MCTS, PPO, self-play |
| `/kaggle-tpu` | TPU et TensorFlow â€” tf.distribute, TFRecords |
| `/kaggle-efficiency` | Vitesse et mÃ©moire â€” reduce_mem, Polars, GPU, caching |
| `/kaggle-ethics` | Biais et fairness â€” audit Ã©thique, Model Cards |

---

## Annexe A â€” Convention d'artefacts

> Cette section est technique. Tu n'as pas besoin de la lire pour utiliser les skills â€” ils gÃ¨rent le nommage automatiquement. C'est utile si tu veux comprendre oÃ¹ les fichiers sont sauvegardÃ©s.

<details>
<summary>Cliquer pour voir la structure des fichiers</summary>

```
mon-projet/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes (train.csv, test.csv)
â”‚   â””â”€â”€ processed/              # DonnÃ©es transformÃ©es
â”œâ”€â”€ reports/                    # Rapports de chaque skill/agent
â”‚   â”œâ”€â”€ strategy/               #   â†’ kaggle-strategist
â”‚   â”œâ”€â”€ research/               #   â†’ kaggle-researcher
â”‚   â”œâ”€â”€ eda/                    #   â†’ /kaggle-eda
â”‚   â”œâ”€â”€ cleaning/               #   â†’ /kaggle-cleaning
â”‚   â”œâ”€â”€ debug/                  #   â†’ kaggle-debugger
â”‚   â”œâ”€â”€ optimizer/              #   â†’ kaggle-optimizer
â”‚   â””â”€â”€ ...                     #   â†’ autres skills
â”œâ”€â”€ artifacts/                  # PrÃ©dictions OOF et test
â”‚   â”œâ”€â”€ oof_lgbm_v1.parquet     #   â†’ prÃ©dictions OOF du LightGBM v1
â”‚   â”œâ”€â”€ test_lgbm_v1.parquet    #   â†’ prÃ©dictions test du LightGBM v1
â”‚   â””â”€â”€ oof_ensemble_v1.parquet #   â†’ OOF de l'ensemble
â”œâ”€â”€ models/                     # ModÃ¨les sauvegardÃ©s (.pkl, .cbm)
â”œâ”€â”€ configs/                    # ParamÃ¨tres optimisÃ©s (.yaml)
â”œâ”€â”€ submissions/                # Fichiers de soumission (.csv)
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter
â””â”€â”€ runs.csv                    # Historique de toutes tes expÃ©riences
```

### Nommage des fichiers

| Type | Format | Exemple |
|------|--------|---------|
| Rapport | `reports/<skill>/YYYY-MM-DD_<desc>.md` | `reports/debug/2026-02-25_score-drop.md` |
| OOF | `artifacts/oof_<model>_v<N>.parquet` | `artifacts/oof_lgbm_v3.parquet` |
| Test | `artifacts/test_<model>_v<N>.parquet` | `artifacts/test_xgb_v2.parquet` |
| Soumission | `submissions/sub_<desc>_<date>.csv` | `submissions/sub_ensemble_2026-02-25.csv` |
| ModÃ¨le | `models/<model>_fold<N>.pkl` | `models/lgbm_fold2.pkl` |
| Config | `configs/<model>_optimized.yaml` | `configs/lgbm_optimized.yaml` |

</details>

---

## Annexe B â€” Definition of Done (contrats de sortie)

> Cette section est pour les utilisateurs avancÃ©s. Elle dÃ©finit quand un skill a VRAIMENT terminÃ© son travail.

<details>
<summary>Cliquer pour voir les contrats de sortie</summary>

| Skill | C'est fini quand... |
|-------|---------------------|
| `/kaggle-baseline` | `runs.csv` (1re ligne) + OOF + soumission + score CV notÃ© |
| `/kaggle-model` | 5 folds OU 2+ seeds + OOF + test predictions + params dans `runs.csv` |
| `/kaggle-ensemble` | Matrice de corrÃ©lation + mÃ©thode documentÃ©e + OOF/test ensemble |
| `/kaggle-metrics` | Fonction mÃ©trique + test mini-sample + vÃ©rification vs LB |
| `/kaggle-postprocess` | Fit sur OOF uniquement + gain mesurÃ© + postprocessor sauvegardÃ© |
| `/kaggle-sanity` | Tous les checks passÃ©s (OK/KO par item) |
| `/kaggle-calibration` | Reliability diagram + ECE avant/aprÃ¨s + calibrateur sauvegardÃ© |
| `kaggle-debugger` | Rapport + patch plan (fichier + ligne + diff) + vÃ©rifications |

</details>
