{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Modeling\n",
    "\n",
    "Multi-model approach: LightGBM, XGBoost, CatBoost, Random Forest, Logistic Regression  \n",
    "**Metric:** Accuracy | **CV:** 5-Fold Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered data\n",
    "train_fe = pd.read_csv('../data/train_fe.csv')\n",
    "test_fe = pd.read_csv('../data/test_fe.csv')\n",
    "\n",
    "features = [c for c in train_fe.columns if c not in ['PassengerId', 'Survived']]\n",
    "\n",
    "X = train_fe[features].values\n",
    "y = train_fe['Survived'].values.astype(int)\n",
    "X_test = test_fe[features].values\n",
    "test_ids = test_fe['PassengerId'].values\n",
    "\n",
    "print(f'X: {X.shape}, y: {y.shape}, X_test: {X_test.shape}')\n",
    "print(f'Features: {len(features)}')\n",
    "print(f'Target: {np.bincount(y)} (0={np.mean(y==0):.2%}, 1={np.mean(y==1):.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV helper\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "def evaluate_model(model, X, y, model_name='Model'):\n",
    "    \"\"\"Evaluate model with stratified K-Fold CV.\"\"\"\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    oof_probs = np.zeros(len(y))\n",
    "    test_preds = np.zeros(X_test.shape[0])\n",
    "    test_probs = np.zeros(X_test.shape[0])\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        val_pred = model.predict(X_val)\n",
    "        oof_preds[val_idx] = val_pred\n",
    "\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            val_prob = model.predict_proba(X_val)[:, 1]\n",
    "            oof_probs[val_idx] = val_prob\n",
    "            test_probs += model.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "\n",
    "        test_preds += model.predict(X_test) / N_FOLDS\n",
    "\n",
    "        fold_acc = accuracy_score(y_val, val_pred)\n",
    "        fold_scores.append(fold_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_scores)\n",
    "    std_acc = np.std(fold_scores)\n",
    "    print(f'{model_name}: CV Accuracy = {mean_acc:.5f} (+/- {std_acc:.5f})')\n",
    "    print(f'  Folds: {[f\"{s:.4f}\" for s in fold_scores]}')\n",
    "\n",
    "    return oof_preds, oof_probs, test_preds, test_probs, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# Logistic Regression (needs scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(C=1.0, max_iter=1000, random_state=SEED)\n",
    "oof_lr, probs_lr, test_lr, tprobs_lr, acc_lr = evaluate_model(lr, X_scaled, y, 'LogisticRegression')\n",
    "results['LR'] = {'oof': probs_lr, 'test': tprobs_lr, 'acc': acc_lr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500, max_depth=8, min_samples_split=5,\n",
    "    min_samples_leaf=2, max_features='sqrt',\n",
    "    random_state=SEED, n_jobs=-1\n",
    ")\n",
    "oof_rf, probs_rf, test_rf, tprobs_rf, acc_rf = evaluate_model(rf, X, y, 'RandomForest')\n",
    "results['RF'] = {'oof': probs_rf, 'test': tprobs_rf, 'acc': acc_rf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=500, max_depth=4, learning_rate=0.05,\n",
    "    subsample=0.8, min_samples_split=5,\n",
    "    random_state=SEED\n",
    ")\n",
    "oof_gb, probs_gb, test_gb, tprobs_gb, acc_gb = evaluate_model(gb, X, y, 'GradientBoosting')\n",
    "results['GB'] = {'oof': probs_gb, 'test': tprobs_gb, 'acc': acc_gb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM with Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1,\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lgb')\n",
    "study_lgb.optimize(lgb_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest LGB CV: {study_lgb.best_value:.5f}')\n",
    "print(f'Best params: {study_lgb.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM with best params\n",
    "best_lgb_params = study_lgb.best_params\n",
    "best_lgb_params.update({\n",
    "    'objective': 'binary', 'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt', 'n_estimators': 1000,\n",
    "    'verbose': -1, 'random_state': SEED\n",
    "})\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**best_lgb_params)\n",
    "oof_lgb, probs_lgb, test_lgb, tprobs_lgb, acc_lgb = evaluate_model(lgb_model, X, y, 'LightGBM (tuned)')\n",
    "results['LGB'] = {'oof': probs_lgb, 'test': tprobs_lgb, 'acc': acc_lgb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost with Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': SEED,\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgb')\n",
    "study_xgb.optimize(xgb_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest XGB CV: {study_xgb.best_value:.5f}')\n",
    "print(f'Best params: {study_xgb.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost with best params\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({\n",
    "    'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "    'n_estimators': 1000, 'random_state': SEED, 'verbosity': 0\n",
    "})\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**best_xgb_params)\n",
    "oof_xgb, probs_xgb, test_xgb, tprobs_xgb, acc_xgb = evaluate_model(xgb_model, X, y, 'XGBoost (tuned)')\n",
    "results['XGB'] = {'oof': probs_xgb, 'test': tprobs_xgb, 'acc': acc_xgb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CatBoost with Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_objective(trial):\n",
    "    params = {\n",
    "        'iterations': 1000,\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'early_stopping_rounds': 50,\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=0)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "study_cb = optuna.create_study(direction='maximize', study_name='cb')\n",
    "study_cb.optimize(cb_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest CatBoost CV: {study_cb.best_value:.5f}')\n",
    "print(f'Best params: {study_cb.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost with best params\n",
    "best_cb_params = study_cb.best_params\n",
    "best_cb_params.update({\n",
    "    'iterations': 1000, 'random_seed': SEED,\n",
    "    'verbose': 0, 'early_stopping_rounds': 50\n",
    "})\n",
    "\n",
    "cb_model = CatBoostClassifier(**best_cb_params)\n",
    "oof_cb, probs_cb, test_cb, tprobs_cb, acc_cb = evaluate_model(cb_model, X, y, 'CatBoost (tuned)')\n",
    "results['CB'] = {'oof': probs_cb, 'test': tprobs_cb, 'acc': acc_cb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=SEED)\n",
    "oof_svm, probs_svm, test_svm, tprobs_svm, acc_svm = evaluate_model(svm, X_scaled, y, 'SVM')\n",
    "results['SVM'] = {'oof': probs_svm, 'test': tprobs_svm, 'acc': acc_svm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV Accuracy': [results[m]['acc'] for m in results]\n",
    "}).sort_values('CV Accuracy', ascending=False)\n",
    "\n",
    "print('=== Model Comparison ===')\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.barh(comparison['Model'], comparison['CV Accuracy'], color='#3498db')\n",
    "ax.set_xlabel('CV Accuracy')\n",
    "ax.set_title('Model Comparison')\n",
    "ax.set_xlim(comparison['CV Accuracy'].min() - 0.02, comparison['CV Accuracy'].max() + 0.01)\n",
    "for bar, acc in zip(bars, comparison['CV Accuracy']):\n",
    "    ax.text(acc + 0.001, bar.get_y() + bar.get_height()/2, f'{acc:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (LightGBM)\n",
    "lgb_final = lgb.LGBMClassifier(**best_lgb_params)\n",
    "lgb_final.fit(X, y)\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': lgb_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.barplot(data=imp, x='importance', y='feature', ax=ax, color='#3498db')\n",
    "ax.set_title('LightGBM Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(imp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF predictions and test predictions for ensemble\n",
    "oof_df = pd.DataFrame({\n",
    "    'PassengerId': train_fe['PassengerId'],\n",
    "    'Survived': y,\n",
    "})\n",
    "test_df = pd.DataFrame({'PassengerId': test_ids})\n",
    "\n",
    "for name in results:\n",
    "    oof_df[f'prob_{name}'] = results[name]['oof']\n",
    "    test_df[f'prob_{name}'] = results[name]['test']\n",
    "\n",
    "oof_df.to_csv('../data/oof_predictions.csv', index=False)\n",
    "test_df.to_csv('../data/test_predictions.csv', index=False)\n",
    "\n",
    "print('Saved OOF and test predictions for ensemble.')\n",
    "print(f'Models: {list(results.keys())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
