{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaceship Titanic - V6 (Optuna Hyperparameter Optimization)\n",
    "\n",
    "**Base:** V2 features (29 features, best LB 0.80710)  \n",
    "**Goal:** Optuna 200 trials per model on LightGBM, XGBoost, CatBoost  \n",
    "**Strategy:**\n",
    "- 5-fold CV for Optuna (speed)\n",
    "- 10-fold CV for final training (stability)\n",
    "- Early stopping in each trial\n",
    "- Simple average ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "import warnings, os, time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS_OPTUNA = 5   # faster for optimization\n",
    "N_FOLDS_FINAL = 10   # stable for final training\n",
    "N_TRIALS = 200\n",
    "TARGET = 'Transported'\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "print(f'V6 Setup: {N_TRIALS} trials per model, {N_FOLDS_OPTUNA}-fold optuna, {N_FOLDS_FINAL}-fold final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_sub = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test[TARGET] = np.nan\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "print(f'Combined: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 Feature Engineering (identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === V2 FE (copy-paste from V2) ===\n",
    "df['_Group'] = df['PassengerId'].str.split('_').str[0].astype(int)\n",
    "df['GroupSize'] = df.groupby('_Group')['PassengerId'].transform('count')\n",
    "df['IsAlone'] = (df['GroupSize'] == 1).astype(int)\n",
    "\n",
    "df['Deck'] = df['Cabin'].str.split('/').str[0]\n",
    "df['CabinNum'] = df['Cabin'].str.split('/').str[1].astype(float)\n",
    "df['Side'] = df['Cabin'].str.split('/').str[2]\n",
    "df['CabinRegion'] = (df['CabinNum'] // 100).astype(float)\n",
    "\n",
    "df['Surname'] = df['Name'].str.split().str[-1]\n",
    "df['FamilySize'] = df.groupby('Surname')['PassengerId'].transform('count')\n",
    "df.loc[df['Surname'].isna(), 'FamilySize'] = 1\n",
    "\n",
    "df['CryoSleep'] = df['CryoSleep'].map({True: 1, False: 0, 'True': 1, 'False': 0})\n",
    "df['VIP'] = df['VIP'].map({True: 1, False: 0, 'True': 1, 'False': 0})\n",
    "\n",
    "# Imputation\n",
    "for col in spend_cols:\n",
    "    mask = (df['CryoSleep'] == 1) & (df[col].isna())\n",
    "    df.loc[mask, col] = 0\n",
    "\n",
    "mask = (df['CryoSleep'].isna()) & (df[spend_cols].sum(axis=1) == 0)\n",
    "df.loc[mask, 'CryoSleep'] = 1\n",
    "mask = (df['CryoSleep'].isna()) & (df[spend_cols].sum(axis=1) > 0)\n",
    "df.loc[mask, 'CryoSleep'] = 0\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Spending\n",
    "df['TotalSpend'] = df[spend_cols].sum(axis=1)\n",
    "df['TotalSpend_log'] = np.log1p(df['TotalSpend'])\n",
    "df['NoSpend'] = (df['TotalSpend'] == 0).astype(int)\n",
    "df['NumServicesUsed'] = (df[spend_cols] > 0).sum(axis=1)\n",
    "for col in spend_cols:\n",
    "    df[f'{col}_log'] = np.log1p(df[col])\n",
    "df['LuxurySpend'] = np.log1p(df['Spa'] + df['VRDeck'] + df['RoomService'])\n",
    "df['BasicSpend'] = np.log1p(df['FoodCourt'] + df['ShoppingMall'])\n",
    "\n",
    "# Age\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 5, 12, 18, 30, 50, 80], labels=[0,1,2,3,4,5]).astype(float)\n",
    "df['IsChild'] = (df['Age'] < 18).astype(float)\n",
    "\n",
    "# Interactions\n",
    "df['CryoSleep_NoSpend'] = ((df['CryoSleep'] == 1) & (df['TotalSpend'] == 0)).astype(int)\n",
    "\n",
    "# Group spending\n",
    "df['GroupSpend_mean'] = df.groupby('_Group')['TotalSpend'].transform('mean')\n",
    "df['GroupSpend_mean_log'] = np.log1p(df['GroupSpend_mean'])\n",
    "\n",
    "# Encodings\n",
    "for col in ['HomePlanet', 'Destination', 'Deck', 'Side']:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_le'] = le.fit_transform(df[col].astype(str))\n",
    "for col in ['HomePlanet', 'Destination', 'Deck', 'Side']:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    df[col + '_freq'] = df[col].map(freq)\n",
    "\n",
    "print('V2 FE done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 features (identical)\n",
    "drop_cols = [\n",
    "    'PassengerId', 'Name', 'Cabin', 'Surname', 'is_train', TARGET,\n",
    "    'HomePlanet', 'Destination', 'Deck', 'Side',\n",
    "    '_Group', 'CabinNum',\n",
    "    'TotalSpend', 'GroupSpend_mean',\n",
    "] + spend_cols\n",
    "\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "print(f'V2 features: {len(features)}')\n",
    "\n",
    "train_df = df[df['is_train'] == 1].copy()\n",
    "test_df = df[df['is_train'] == 0].copy()\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df[TARGET].astype(int).values\n",
    "X_test = test_df[features].values\n",
    "\n",
    "print(f'X: {X.shape}, y: {y.shape}, X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna: LightGBM (200 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 64),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n",
    "        'n_estimators': 5000,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=N_FOLDS_OPTUNA, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        acc = accuracy_score(y_val, (preds > 0.5).astype(int))\n",
    "        scores.append(acc)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(f'Starting LightGBM Optuna: {N_TRIALS} trials...')\n",
    "t0 = time.time()\n",
    "\n",
    "lgb_study = optuna.create_study(direction='maximize', study_name='lgb')\n",
    "lgb_study.optimize(lgb_objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print(f'\\nLightGBM Optuna done in {time.time()-t0:.0f}s')\n",
    "print(f'Best CV Accuracy: {lgb_study.best_value:.5f}')\n",
    "print(f'Best params:')\n",
    "for k, v in lgb_study.best_params.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna: XGBoost (200 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.95),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 2.0),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 5),\n",
    "        'n_estimators': 5000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': SEED,\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=N_FOLDS_OPTUNA, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=0)\n",
    "        \n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        acc = accuracy_score(y_val, (preds > 0.5).astype(int))\n",
    "        scores.append(acc)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(f'Starting XGBoost Optuna: {N_TRIALS} trials...')\n",
    "t0 = time.time()\n",
    "\n",
    "xgb_study = optuna.create_study(direction='maximize', study_name='xgb')\n",
    "xgb_study.optimize(xgb_objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print(f'\\nXGBoost Optuna done in {time.time()-t0:.0f}s')\n",
    "print(f'Best CV Accuracy: {xgb_study.best_value:.5f}')\n",
    "print(f'Best params:')\n",
    "for k, v in xgb_study.best_params.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna: CatBoost (200 trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_objective(trial):\n",
    "    params = {\n",
    "        'iterations': 5000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 0.95),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.4, 0.95),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.0, 5.0),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 5.0),\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'task_type': 'CPU',\n",
    "    }\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=N_FOLDS_OPTUNA, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "        \n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        acc = accuracy_score(y_val, (preds > 0.5).astype(int))\n",
    "        scores.append(acc)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(f'Starting CatBoost Optuna: {N_TRIALS} trials...')\n",
    "t0 = time.time()\n",
    "\n",
    "cb_study = optuna.create_study(direction='maximize', study_name='catboost')\n",
    "cb_study.optimize(cb_objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print(f'\\nCatBoost Optuna done in {time.time()-t0:.0f}s')\n",
    "print(f'Best CV Accuracy: {cb_study.best_value:.5f}')\n",
    "print(f'Best params:')\n",
    "for k, v in cb_study.best_params.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('OPTUNA OPTIMIZATION SUMMARY')\n",
    "print('='*60)\n",
    "print(f'LightGBM best 5-fold CV: {lgb_study.best_value:.5f}')\n",
    "print(f'XGBoost  best 5-fold CV: {xgb_study.best_value:.5f}')\n",
    "print(f'CatBoost best 5-fold CV: {cb_study.best_value:.5f}')\n",
    "print(f'\\nV2 baseline 10-fold CV:  0.81410 (simple avg ensemble)')\n",
    "\n",
    "# Optimization history plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, study, name in zip(axes, [lgb_study, xgb_study, cb_study], ['LightGBM', 'XGBoost', 'CatBoost']):\n",
    "    trials = study.trials\n",
    "    values = [t.value for t in trials if t.value is not None]\n",
    "    best_so_far = np.maximum.accumulate(values)\n",
    "    ax.plot(values, alpha=0.3, label='Trial score')\n",
    "    ax.plot(best_so_far, color='red', linewidth=2, label='Best so far')\n",
    "    ax.set_title(f'{name} - Best: {study.best_value:.5f}')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle(f'Optuna Optimization ({N_TRIALS} trials each)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training with Optimized Params (10-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LightGBM with best params ===\n",
    "best_lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 5000,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': SEED,\n",
    "    **lgb_study.best_params\n",
    "}\n",
    "\n",
    "oof_lgb = np.zeros(len(X))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "fi_lgb = np.zeros(len(features))\n",
    "\n",
    "kf = StratifiedKFold(n_splits=N_FOLDS_FINAL, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**best_lgb_params)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(500)]\n",
    "    )\n",
    "    \n",
    "    oof_lgb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_lgb += model.predict_proba(X_test)[:, 1] / N_FOLDS_FINAL\n",
    "    fi_lgb += model.feature_importances_ / N_FOLDS_FINAL\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_lgb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS_FINAL} - Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "lgb_acc = accuracy_score(y, (oof_lgb > 0.5).astype(int))\n",
    "print(f'\\nLightGBM OPTIMIZED 10-fold CV: {lgb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === XGBoost with best params ===\n",
    "best_xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'n_estimators': 5000,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': SEED,\n",
    "    'verbosity': 0,\n",
    "    **xgb_study.best_params\n",
    "}\n",
    "\n",
    "oof_xgb = np.zeros(len(X))\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**best_xgb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=500)\n",
    "    \n",
    "    oof_xgb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_xgb += model.predict_proba(X_test)[:, 1] / N_FOLDS_FINAL\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_xgb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS_FINAL} - Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "xgb_acc = accuracy_score(y, (oof_xgb > 0.5).astype(int))\n",
    "print(f'\\nXGBoost OPTIMIZED 10-fold CV: {xgb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CatBoost with best params ===\n",
    "best_cb_params = {\n",
    "    'iterations': 5000,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 500,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'task_type': 'CPU',\n",
    "    **cb_study.best_params\n",
    "}\n",
    "\n",
    "oof_cb = np.zeros(len(X))\n",
    "test_cb = np.zeros(len(X_test))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(**best_cb_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "    \n",
    "    oof_cb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_cb += model.predict_proba(X_test)[:, 1] / N_FOLDS_FINAL\n",
    "    \n",
    "    fold_acc = accuracy_score(y_val, (oof_cb[val_idx] > 0.5).astype(int))\n",
    "    print(f'Fold {fold+1}/{N_FOLDS_FINAL} - Accuracy: {fold_acc:.5f}')\n",
    "\n",
    "cb_acc = accuracy_score(y, (oof_cb > 0.5).astype(int))\n",
    "print(f'\\nCatBoost OPTIMIZED 10-fold CV: {cb_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== V6 OPTIMIZED Results ===')\n",
    "print(f'LightGBM: {lgb_acc:.5f}')\n",
    "print(f'XGBoost:  {xgb_acc:.5f}')\n",
    "print(f'CatBoost: {cb_acc:.5f}')\n",
    "\n",
    "# Simple average\n",
    "oof_avg = (oof_lgb + oof_xgb + oof_cb) / 3\n",
    "avg_acc = accuracy_score(y, (oof_avg > 0.5).astype(int))\n",
    "print(f'\\nSimple Average: {avg_acc:.5f}')\n",
    "\n",
    "# Majority voting\n",
    "votes = ((oof_lgb > 0.5).astype(int) + (oof_xgb > 0.5).astype(int) + (oof_cb > 0.5).astype(int))\n",
    "vote_acc = accuracy_score(y, (votes >= 2).astype(int))\n",
    "print(f'Majority Voting: {vote_acc:.5f}')\n",
    "\n",
    "# Submission\n",
    "final_proba = (test_lgb + test_xgb + test_cb) / 3\n",
    "final_preds = (final_proba > 0.5)\n",
    "\n",
    "print(f'\\nTest: {final_preds.sum()} True / {len(final_preds) - final_preds.sum()} False')\n",
    "print(f'Ratio: {final_preds.mean():.4f}')\n",
    "\n",
    "print(f'\\n=== VERSION COMPARISON ===')\n",
    "print(f'V1:  CV 0.82653 | LB 0.80196 | Gap 0.0246 | 49 feat | manual params')\n",
    "print(f'V2:  CV 0.81410 | LB 0.80710 | Gap 0.0070 | 29 feat | manual params')\n",
    "print(f'V3:  CV 0.81836 | LB 0.80406 | Gap 0.0143 | 56 feat | manual params')\n",
    "print(f'V5:  CV 0.81767 | LB TBD     |            | 32 feat | manual params + TE')\n",
    "print(f'V6:  CV {avg_acc:.5f} | LB TBD     |            | 29 feat | OPTUNA {N_TRIALS} trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "fi_df = pd.DataFrame({'feature': features, 'importance': fi_lgb})\n",
    "fi_df = fi_df.sort_values('importance', ascending=True).tail(20)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(fi_df['feature'], fi_df['importance'], color='steelblue')\n",
    "plt.title(f'LightGBM V6 (Optuna) Feature Importance - CV: {lgb_acc:.5f}')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'].values,\n",
    "    'Transported': final_preds\n",
    "})\n",
    "submission['Transported'] = submission['Transported'].astype(bool)\n",
    "submission.to_csv('../submissions/submission_v6.csv', index=False)\n",
    "\n",
    "assert submission.shape[0] == sample_sub.shape[0]\n",
    "assert list(submission.columns) == list(sample_sub.columns)\n",
    "assert submission['Transported'].dtype == bool\n",
    "\n",
    "print('V6 Submission saved: submissions/submission_v6.csv')\n",
    "print(submission['Transported'].value_counts(normalize=True))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best params for future reference\n",
    "print('\\n' + '='*60)\n",
    "print('BEST HYPERPARAMETERS (save for future use)')\n",
    "print('='*60)\n",
    "print(f'\\n# LightGBM best params:')\n",
    "for k, v in lgb_study.best_params.items():\n",
    "    print(f\"    '{k}': {repr(v)},\")\n",
    "print(f'\\n# XGBoost best params:')\n",
    "for k, v in xgb_study.best_params.items():\n",
    "    print(f\"    '{k}': {repr(v)},\")\n",
    "print(f'\\n# CatBoost best params:')\n",
    "for k, v in cb_study.best_params.items():\n",
    "    print(f\"    '{k}': {repr(v)},\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
