{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Ensemble & Submission\n",
    "\n",
    "Multiple ensemble strategies:\n",
    "1. Weighted Average (optimized weights)\n",
    "2. Rank Average\n",
    "3. Majority Voting\n",
    "4. Stacking (Logistic Regression meta-learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions from modeling notebook\n",
    "oof_df = pd.read_csv('../data/oof_predictions.csv')\n",
    "test_df = pd.read_csv('../data/test_predictions.csv')\n",
    "sample_sub = pd.read_csv('../data/gender_submission.csv')\n",
    "\n",
    "y = oof_df['Survived'].values.astype(int)\n",
    "model_cols = [c for c in oof_df.columns if c.startswith('prob_')]\n",
    "model_names = [c.replace('prob_', '') for c in model_cols]\n",
    "\n",
    "oof_probs = oof_df[model_cols].values\n",
    "test_probs = test_df[model_cols].values\n",
    "test_ids = test_df['PassengerId'].values\n",
    "\n",
    "print(f'Models: {model_names}')\n",
    "print(f'OOF shape: {oof_probs.shape}, Test shape: {test_probs.shape}')\n",
    "\n",
    "# Individual model CV scores\n",
    "print('\\n=== Individual Model CV Scores ===')\n",
    "for i, name in enumerate(model_names):\n",
    "    preds = (oof_probs[:, i] > THRESHOLD).astype(int)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    print(f'{name}: {acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple average\n",
    "oof_avg = oof_probs.mean(axis=1)\n",
    "test_avg = test_probs.mean(axis=1)\n",
    "\n",
    "avg_acc = accuracy_score(y, (oof_avg > THRESHOLD).astype(int))\n",
    "print(f'Simple Average CV Accuracy: {avg_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimized Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal weights using scipy minimize\n",
    "def neg_accuracy(weights):\n",
    "    weights = np.abs(weights) / np.sum(np.abs(weights))  # normalize\n",
    "    blend = np.dot(oof_probs, weights)\n",
    "    preds = (blend > THRESHOLD).astype(int)\n",
    "    return -accuracy_score(y, preds)\n",
    "\n",
    "n_models = len(model_names)\n",
    "initial_weights = np.ones(n_models) / n_models\n",
    "\n",
    "# Multiple random restarts\n",
    "best_result = None\n",
    "best_score = -1\n",
    "\n",
    "for _ in range(100):\n",
    "    w0 = np.random.dirichlet(np.ones(n_models))\n",
    "    result = minimize(neg_accuracy, w0, method='Nelder-Mead',\n",
    "                      options={'maxiter': 10000})\n",
    "    if -result.fun > best_score:\n",
    "        best_score = -result.fun\n",
    "        best_result = result\n",
    "\n",
    "opt_weights = np.abs(best_result.x) / np.sum(np.abs(best_result.x))\n",
    "\n",
    "oof_weighted = np.dot(oof_probs, opt_weights)\n",
    "test_weighted = np.dot(test_probs, opt_weights)\n",
    "\n",
    "weighted_acc = accuracy_score(y, (oof_weighted > THRESHOLD).astype(int))\n",
    "print(f'Optimized Weighted Average CV: {weighted_acc:.5f}')\n",
    "print(f'\\nOptimal weights:')\n",
    "for name, w in zip(model_names, opt_weights):\n",
    "    print(f'  {name}: {w:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rank Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank average - robust to different probability scales\n",
    "oof_ranks = np.column_stack([\n",
    "    rankdata(oof_probs[:, i]) / len(y) for i in range(n_models)\n",
    "])\n",
    "test_ranks = np.column_stack([\n",
    "    rankdata(test_probs[:, i]) / len(test_ids) for i in range(n_models)\n",
    "])\n",
    "\n",
    "oof_rank_avg = oof_ranks.mean(axis=1)\n",
    "test_rank_avg = test_ranks.mean(axis=1)\n",
    "\n",
    "rank_acc = accuracy_score(y, (oof_rank_avg > THRESHOLD).astype(int))\n",
    "print(f'Rank Average CV Accuracy: {rank_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard voting (majority)\n",
    "oof_votes = (oof_probs > THRESHOLD).astype(int)\n",
    "test_votes = (test_probs > THRESHOLD).astype(int)\n",
    "\n",
    "oof_majority = (oof_votes.mean(axis=1) > 0.5).astype(int)\n",
    "test_majority = (test_votes.mean(axis=1) > 0.5).astype(int)\n",
    "\n",
    "vote_acc = accuracy_score(y, oof_majority)\n",
    "print(f'Majority Voting CV Accuracy: {vote_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking (Meta-learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking with Logistic Regression meta-learner\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "oof_stack = np.zeros(len(y))\n",
    "test_stack = np.zeros(len(test_ids))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(oof_probs, y)):\n",
    "    X_tr = oof_probs[train_idx]\n",
    "    y_tr = y[train_idx]\n",
    "    X_val = oof_probs[val_idx]\n",
    "\n",
    "    meta = LogisticRegression(C=1.0, random_state=SEED, max_iter=1000)\n",
    "    meta.fit(X_tr, y_tr)\n",
    "\n",
    "    oof_stack[val_idx] = meta.predict_proba(X_val)[:, 1]\n",
    "    test_stack += meta.predict_proba(test_probs)[:, 1] / 5\n",
    "\n",
    "stack_acc = accuracy_score(y, (oof_stack > THRESHOLD).astype(int))\n",
    "print(f'Stacking CV Accuracy: {stack_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold for the best ensemble\n",
    "ensembles = {\n",
    "    'Simple Average': (oof_avg, test_avg),\n",
    "    'Weighted Average': (oof_weighted, test_weighted),\n",
    "    'Rank Average': (oof_rank_avg, test_rank_avg),\n",
    "    'Stacking': (oof_stack, test_stack),\n",
    "}\n",
    "\n",
    "print('=== Ensemble Comparison ===')\n",
    "best_ensemble_name = None\n",
    "best_ensemble_acc = 0\n",
    "\n",
    "for name, (oof, test) in ensembles.items():\n",
    "    # Find optimal threshold\n",
    "    best_thr = 0.5\n",
    "    best_acc = 0\n",
    "    for thr in np.arange(0.3, 0.7, 0.01):\n",
    "        acc = accuracy_score(y, (oof > thr).astype(int))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_thr = thr\n",
    "\n",
    "    print(f'{name}: {best_acc:.5f} (threshold={best_thr:.2f})')\n",
    "\n",
    "    if best_acc > best_ensemble_acc:\n",
    "        best_ensemble_acc = best_acc\n",
    "        best_ensemble_name = name\n",
    "\n",
    "print(f'\\nBest ensemble: {best_ensemble_name} ({best_ensemble_acc:.5f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all submissions\n",
    "import os\n",
    "os.makedirs('../submissions', exist_ok=True)\n",
    "\n",
    "submissions = {\n",
    "    'weighted_avg': (test_weighted > THRESHOLD).astype(int),\n",
    "    'rank_avg': (test_rank_avg > THRESHOLD).astype(int),\n",
    "    'majority_vote': test_majority,\n",
    "    'stacking': (test_stack > THRESHOLD).astype(int),\n",
    "    'simple_avg': (test_avg > THRESHOLD).astype(int),\n",
    "}\n",
    "\n",
    "for name, preds in submissions.items():\n",
    "    sub = pd.DataFrame({\n",
    "        'PassengerId': test_ids.astype(int),\n",
    "        'Survived': preds.astype(int)\n",
    "    })\n",
    "    filepath = f'../submissions/submission_{name}.csv'\n",
    "    sub.to_csv(filepath, index=False)\n",
    "    print(f'{name}: {sub.Survived.mean():.3f} survival rate, saved to {filepath}')\n",
    "\n",
    "print(f'\\n{len(submissions)} submission files created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check best submission\n",
    "best_sub = pd.read_csv(f'../submissions/submission_weighted_avg.csv')\n",
    "print('=== Best Submission Sanity Check ===')\n",
    "print(f'Shape: {best_sub.shape}')\n",
    "print(f'Expected shape: {sample_sub.shape}')\n",
    "print(f'Columns match: {list(best_sub.columns) == list(sample_sub.columns)}')\n",
    "print(f'PassengerId range: {best_sub.PassengerId.min()} - {best_sub.PassengerId.max()}')\n",
    "print(f'Survived values: {best_sub.Survived.unique()}')\n",
    "print(f'Survival rate: {best_sub.Survived.mean():.3f}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(best_sub.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Overview\n",
    "1. **EDA** -> Key insights: Sex, Pclass, Title, Family Size\n",
    "2. **Feature Engineering** -> 24 features from 11 original columns\n",
    "3. **Modeling** -> 6 models with Optuna-tuned GBDT\n",
    "4. **Ensemble** -> 5 ensemble strategies\n",
    "\n",
    "### Next Steps for Higher Score\n",
    "- Add name-based group survival features (women-children-first rule)\n",
    "- Try neural network (small MLP)\n",
    "- External data (deck plans, passenger lists)\n",
    "- Pseudo-labeling with confident test predictions\n",
    "- Manual rule adjustments for edge cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
